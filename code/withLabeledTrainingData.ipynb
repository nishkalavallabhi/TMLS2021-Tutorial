{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming I already have some amount of labeled training data, what can I do?\n",
    "\n",
    "Here are a few options:\n",
    "1. I can train my own classification model using any kind of feature extraction, and choosing from a range of available ML/DL algorithms. \n",
    "2. I can take an existing Deep learning model and \"fine-tune\" it for my task.\n",
    "\n",
    "I will use the train_labelled.txt file as training data and , and use Sentence Transformers (sbert.net) to get a feature representation for the text. I will use Logistic regression as my classifier.\n",
    "\n",
    "Note that my goal here is not to exhaustively evaluate how to train, but to only illustrate how training our own model (with a not so huge labeled dataset) compare with using a cloud provider's solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Following a typical text classification approach (Bag of words features + logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76       500\n",
      "           1       0.78      0.67      0.72       500\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.75      0.74      0.74      1000\n",
      "weighted avg       0.75      0.74      0.74      1000\n",
      "\n",
      "[[408  92]\n",
      " [166 334]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import string\n",
    "\n",
    "stopwords = ENGLISH_STOP_WORDS\n",
    "# remove stopwords, punctuation and numbers\n",
    "def clean(doc): # doc is a string of text\n",
    "    doc = \"\".join([char for char in doc if char not in string.punctuation and not char.isdigit()])\n",
    "    doc = \" \".join([token for token in doc.split() if token not in stopwords])\n",
    "    return doc\n",
    "\n",
    "#reads train/test files\n",
    "def read_data(filepath):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for line in open(filepath):\n",
    "        sentence, label = line.strip().split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        texts.append(sentence)\n",
    "    return texts, labels\n",
    "    \n",
    "train_texts, train_labels = read_data(\"../files/train_labelled.txt\")\n",
    "test_texts, test_labels = read_data(\"../files/test_labelled.txt\")\n",
    "\n",
    "#BOW feature extraction\n",
    "vect = CountVectorizer(preprocessor=clean, max_features=1000) # instantiate a vectoriezer\n",
    "train_vector = vect.fit_transform(train_texts)# use it to extract features from training data\n",
    "# transform test data (using training data's features)\n",
    "test_vector = vect.transform(test_texts)\n",
    "\n",
    "#Use a logistic regression classifier to train and test the model\n",
    "logreg = LogisticRegression() # instantiate a logistic regression model\n",
    "logreg.fit(train_vector, train_labels) # fit the model with training data\n",
    "# Make predictions on test data\n",
    "predicted = logreg.predict(test_vector)\n",
    "\n",
    "#Print results.\n",
    "print(classification_report(test_labels, predicted))\n",
    "print(confusion_matrix(test_labels,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use a neural text representation, from Sentence Transformers library, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#Choose from the models here: https://www.sbert.net/docs/pretrained_models.html\n",
    "model = SentenceTransformer('paraphrase-TinyBERT-L6-v2')\n",
    "\n",
    "#Read the train/test files and extract labels and \n",
    "#textual features using the sentence transformers model.\n",
    "def read_data(filepath):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for line in open(filepath):\n",
    "        sentence, label = line.strip().split(\"\\t\")\n",
    "        labels.append(label)\n",
    "        data.append(model.encode(sentence)) #feature extraction\n",
    "    return data, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87       500\n",
      "           1       0.86      0.89      0.88       500\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.88      0.88      0.87      1000\n",
      "weighted avg       0.88      0.88      0.87      1000\n",
      "\n",
      "[[428  72]\n",
      " [ 53 447]]\n"
     ]
    }
   ],
   "source": [
    "#feature extraction\n",
    "train_data, train_labels = read_data(\"../files/train_labelled.txt\")\n",
    "test_data, test_labels = read_data(\"../files/test_labelled.txt\")\n",
    "\n",
    "\n",
    "#modeling\n",
    "clf = LogisticRegression(random_state=0).fit(train_data, train_labels)\n",
    "\n",
    "#prediction/evaluation\n",
    "predicted = clf.predict(test_data)\n",
    "print(classification_report(test_labels, predicted))\n",
    "print(confusion_matrix(test_labels,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does all this compare with snorkel's generated labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "snorkel_train = []\n",
    "snorkel_train_labels = []\n",
    "for line in open(\"../files/snorkellabeled_train.csv\"):\n",
    "        sentence, label = line.strip().split(\"\\t\") #this file has an extra id column.\n",
    "        snorkel_train_labels.append(label)\n",
    "        snorkel_train.append(model.encode(sentence)) #feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.49      0.64       500\n",
      "           1       0.65      0.96      0.78       500\n",
      "\n",
      "    accuracy                           0.73      1000\n",
      "   macro avg       0.79      0.73      0.71      1000\n",
      "weighted avg       0.79      0.73      0.71      1000\n",
      "\n",
      "[[247 253]\n",
      " [ 20 480]]\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(snorkel_train, snorkel_train_labels)\n",
    "#prediction/evaluation\n",
    "predicted = clf.predict(test_data)\n",
    "print(classification_report(test_labels, predicted))\n",
    "print(confusion_matrix(test_labels,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
