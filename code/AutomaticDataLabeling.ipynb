{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Data Labeling for Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how can we use automatic data labeling for building a sentiment classifier. This consists of four major steps:\n",
    "\n",
    "1. **Loading Data**: train_labelled.txt, test_labelled.txt are the two files (from files/ folder) we will use here. The source of this data is mentioned in the slides. In the train_labelled.txt, I am only going to use the text, and discard the labels. I am going to keep the labels for test data, as we need something to evaluate our approach.\n",
    "\n",
    "2. **Writing Labeling Functions**: We write Python programs that take as input a data point and assign labels (or abstain) using heuristics, pattern matching, or third-party models.\n",
    "\n",
    "3. **Combining Labeling Function Outputs with the Label Model**: We model the outputs of the labeling functions over the training set using a novel, theoretically-grounded [modeling approach](https://arxiv.org/abs/1605.07723), which estimates the accuracies and correlations of the labeling functions using only their agreements and disagreements, and then uses this to reweight and combine their outputs, which we then use as _probabilistic_ training labels.\n",
    "\n",
    "4. **Training a Classifier**: We train a classifier that can predict labels for *any* YouTube comment (not just the ones labeled by the labeling functions) using the probabilistic training labels from step 3.\n",
    "\n",
    "(Text is adapted from original Snorkel tutorial on [spam classification](https://github.com/snorkel-team/snorkel-tutorials/blob/master/spam/01_spam_tutorial.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads train/test files\n",
    "def read_data(filepath):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for line in open(filepath):\n",
    "        sentence, label = line.strip().split(\"\\t\")\n",
    "        labels.append(int(label))\n",
    "        texts.append(sentence)\n",
    "    return texts, labels\n",
    "    \n",
    "train_texts, discard = read_data(\"../files/train_labelled.txt\")\n",
    "test_texts, test_labels = read_data(\"../files/test_labelled.txt\")\n",
    "discard = None #training labels are discarded. We won't use them\n",
    "\n",
    "#convert to dataframe for ease of use later.\n",
    "from pandas import DataFrame\n",
    "df_train = DataFrame (train_texts,columns=['text'])\n",
    "df_test = DataFrame(test_texts,columns=['text'])\n",
    "df_test['label'] = test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing Labeling Functions (LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Exploring the training set for initial ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by looking at 20 random data points from the `train` set to generate some ideas for LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I love Lane, but I've never seen her in a movie this lousy.  \",\n",
       " 'The original Body and Soul (1947) is a masterpiece.  ',\n",
       " 'The script is\\x85was there a script?  ',\n",
       " 'Our server was super nice and checked on us many times.',\n",
       " 'Very disappointed and wondered how it could be in the Oscar shortlist.  ',\n",
       " \"Clever and a real crowd-pleaser--this film still ranks among Mickey's best films even after 80 wonderful years.  \",\n",
       " 'The plot simply rumbles on like a machine, desperately depending on the addition of new scenes.  ',\n",
       " 'Tonight I had the Elk Filet special...and it sucked.',\n",
       " 'This place deserves no stars.',\n",
       " \"Once your food arrives it's meh.\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(train_texts,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alexander Nevsky is a great film.  ',\n",
       " 'Why was this film made?  ',\n",
       " 'It was not good.',\n",
       " 'This is a great restaurant at the Mandalay Bay.',\n",
       " 'It was also the right balance of war and love.  ',\n",
       " \"It's a mediocre, miserable, hollow, laughable and predictable piece of garbage.  \",\n",
       " 'There was a warm feeling with the service and I felt like their guest for a special treat.',\n",
       " 'The only thing really worth watching was the scenery and the house, because it is beautiful.  ',\n",
       " 'It is not just a cult... it is a cult CLASSIC.  ',\n",
       " 'I thoroughly enjoyed it when Christopher Eccleston took control of the TARDIS and the continuation of the series.  ']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(train_texts,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a list of positive/negative opinion words is a good starting point for writing the labeling functions. I will use the \"Opinion Lexicon\" from [here](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html) for this purpose. This contains a list of English positive and negative opinion words or sentiment words (around 6800 words in total). Details about the data are in the link. negative-words.txt, positive-words.txt in files/ contain these files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Writing a few LFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling functions in Snorkel are created with the\n",
    "[`@labeling_function` decorator](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.labeling_function.html).\n",
    "The [decorator](https://realpython.com/primer-on-python-decorators/) can be applied to _any Python function_ that returns a label for a single data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load positive and negative words lists.\n",
    "def getsentimentwords(filepath):\n",
    "    mylist = []\n",
    "    for line in open(filepath, encoding=\"utf-8\"):\n",
    "        if line and not line.startswith(\";\"):\n",
    "            mylist.append(line.strip())\n",
    "    return mylist\n",
    "\n",
    "positives = getsentimentwords(\"../resources/positive-words.txt\")\n",
    "negatives = getsentimentwords(\"../resources/negative-words.txt\")\n",
    "\n",
    "#load the vader lexicon:\n",
    "def getvader(filepath):\n",
    "    mydict = {}\n",
    "    for line in open(filepath, encoding=\"utf-8\"):\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[0].isalpha():\n",
    "            mydict[temp[0]] = float(temp[1])\n",
    "    return mydict\n",
    "myvader = getvader(\"../resources/vader_lexicon.txt\")\n",
    "\n",
    "#load pos/neg emotions words\n",
    "def getemotions(filepath):\n",
    "    mylist = []\n",
    "    for line in open(filepath, encoding=\"utf-8\"):\n",
    "        mylist.extend(line.strip().split())\n",
    "    return mylist\n",
    "\n",
    "myposemotions = getemotions(\"../resources/posemotions.txt\")\n",
    "mynegemotions = getemotions(\"../resources/negemotions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n",
      "4783\n",
      "7209\n"
     ]
    }
   ],
   "source": [
    "print(len(positives))\n",
    "print(len(negatives))\n",
    "print(len(myvader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "import re\n",
    "\n",
    "POS=1\n",
    "NEG=0\n",
    "ABSTAIN=-1\n",
    "\n",
    "#a simple labeling function checking if a sentence has positive words\n",
    "@labeling_function()\n",
    "def postive(x):\n",
    "    poswords = 0\n",
    "    temp = x.text.lower().split()\n",
    "    for word in temp:\n",
    "        if word in positives:\n",
    "            poswords +=1\n",
    "    if poswords > 0:\n",
    "        return POS\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "#a simple labeling function checking if a sentence has negative words    \n",
    "@labeling_function()\n",
    "def negative(x):\n",
    "    negwords = 0\n",
    "    temp = x.text.lower().split()\n",
    "    for word in temp:\n",
    "        if word in negatives:\n",
    "            negwords +=1\n",
    "    if negwords > 0:\n",
    "        return NEG\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "#Look up the word's mean-sentiment rating in Vader Lexicon\n",
    "#https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt\n",
    "#if overall score for a sentence is positive, return POS, else, NEG. if 0, return ABSTAIN.\n",
    "@labeling_function()\n",
    "def vaderlex(x):\n",
    "    temp = x.text.lower().split()\n",
    "    sentiment = 0\n",
    "    for word in temp:\n",
    "        if word in myvader:\n",
    "            sentiment += myvader[word]\n",
    "    if sentiment > 0:\n",
    "        return POS\n",
    "    elif sentiment <0:\n",
    "        return NEG\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "#Look up the word's mean-sentiment rating in Vader Lexicon\n",
    "#https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt\n",
    "#if there are more positive words (>1.5 rating) it is POS, else if less than -1.5, NEG. \n",
    "#finally, if POS=NEG, return ABSTAIN.\n",
    "@labeling_function()\n",
    "def vaderlex2(x):\n",
    "    temp = x.text.lower().split()\n",
    "    poswords = 0\n",
    "    negwords = 0\n",
    "    for word in temp:\n",
    "        if word in myvader:\n",
    "            sentiment = myvader[word]\n",
    "            if sentiment > 1.5:\n",
    "                poswords +=1\n",
    "            elif sentiment <-1.5:\n",
    "                negwords += 1\n",
    "\n",
    "    if poswords > negwords:\n",
    "        return POS\n",
    "    elif negwords > poswords:\n",
    "        return NEG\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "#a simple labeling function checking if a sentence has positive emotion words\n",
    "@labeling_function()\n",
    "def posemo(x):\n",
    "    poswords = 0\n",
    "    temp = x.text.lower().split()\n",
    "    for word in temp:\n",
    "        if word in myposemotions:\n",
    "            poswords +=1\n",
    "    if poswords > 0:\n",
    "        return POS\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "#a simple labeling function checking if a sentence has negative emotion words\n",
    "@labeling_function()\n",
    "def negemo(x):\n",
    "    poswords = 0\n",
    "    temp = x.text.lower().split()\n",
    "    for word in temp:\n",
    "        if word in mynegemotions:\n",
    "            poswords +=1\n",
    "    if poswords > 0:\n",
    "        return POS\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "#More function ideas: https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply one or more LFs that we've written to a collection of data points, we use an\n",
    "[`LFApplier`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFApplier.html).\n",
    "Because our data points are represented with a Pandas DataFrame in this tutorial, we use the\n",
    "[`PandasLFApplier`](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.PandasLFApplier.html).\n",
    "Correspondingly, a single data point `x` that's passed into our LFs will be a [Pandas `Series` object](https://pandas.pydata.org/pandas-docs/stable/reference/series.html).\n",
    "\n",
    "It's important to note that these LFs will work for any object with an attribute named `text`, not just Pandas objects.\n",
    "Snorkel has several other appliers for different data point collection types which you can browse in the [API documentation](https://snorkel.readthedocs.io/en/master/packages/labeling.html).\n",
    "\n",
    "The output of the `apply(...)` method is a ***label matrix***, a fundamental concept in Snorkel.\n",
    "It's a NumPy array `L` with one column for each LF and one row for each data point, where `L[i, j]` is the label that the `j`th labeling function output for the `i`th data point.\n",
    "We'll create a label matrix for the `train` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 924.68it/s] \n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "from pandas import DataFrame\n",
    "\n",
    "lfs = [postive,negative,vaderlex,vaderlex2,posemo,negemo]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Evaluate performance on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of statistics about labeling functions &mdash; like coverage &mdash; are useful when building any Snorkel application.\n",
    "So Snorkel provides tooling for common LF analyses using the\n",
    "[`LFAnalysis` utility](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.LFAnalysis.html).\n",
    "We report the following summary statistics for multiple LFs at once:\n",
    "\n",
    "* **Polarity**: The set of unique labels this LF outputs (excluding abstains)\n",
    "* **Coverage**: The fraction of the dataset the LF labels\n",
    "* **Overlaps**: The fraction of the dataset where this LF and at least one other LF label\n",
    "* **Conflicts**: The fraction of the dataset where this LF and at least one other LF label and disagree\n",
    "* **Correct**: The number of data points this LF labels correctly (if gold labels are provided)\n",
    "* **Incorrect**: The number of data points this LF labels incorrectly (if gold labels are provided)\n",
    "* **Empirical Accuracy**: The empirical accuracy of this LF (if gold labels are provided)\n",
    "\n",
    "For *Correct*, *Incorrect*, and *Empirical Accuracy*, we don't want to penalize the LF for data points where it abstained.\n",
    "We calculate these statistics only over those data points where the LF output a label.\n",
    "**Note that in our current setup, we can't compute these statistics because we don't have any ground-truth labels (other than in the test set, which we cannot look at). Not to worry—Snorkel's `LabelModel` will estimate them without needing any ground-truth labels in the next step!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>postive</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.4695</td>\n",
       "      <td>0.4415</td>\n",
       "      <td>0.1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.1610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaderlex</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.6320</td>\n",
       "      <td>0.5965</td>\n",
       "      <td>0.1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaderlex2</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.4715</td>\n",
       "      <td>0.4715</td>\n",
       "      <td>0.1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.1985</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>0.0490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.0845</td>\n",
       "      <td>0.0775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           j Polarity  Coverage  Overlaps  Conflicts\n",
       "postive    0      [1]    0.4695    0.4415     0.1055\n",
       "negative   1      [0]    0.2725    0.2370     0.1610\n",
       "vaderlex   2   [0, 1]    0.6320    0.5965     0.1635\n",
       "vaderlex2  3   [0, 1]    0.4715    0.4715     0.1095\n",
       "posemo     4      [1]    0.1985    0.1950     0.0490\n",
       "negemo     5      [1]    0.0865    0.0845     0.0775"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "lfs = [postive,negative, vaderlex, vaderlex2, posemo, negemo]\n",
    "\n",
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining Labeling Function Outputs with the Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates just a handful of the types of LFs that one might write for this task.\n",
    "One of the key goals of Snorkel is _not_ to replace the effort, creativity, and subject matter expertise required to come up with these labeling functions, but rather to make it faster to write them, since **in Snorkel the labeling functions are assumed to be noisy, i.e. innaccurate, overlapping, etc.**\n",
    "Said another way: the LF abstraction provides a flexible interface for conveying a huge variety of supervision signals, and the `LabelModel` is able to denoise these signals, reducing the need for painstaking manual fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we perform some LFs analysis and finalize our list,  we can now apply these once again with `LFApplier` to get the label matrices.\n",
    "The Pandas format provides an easy interface that many practitioners are familiar with, but it is also less optimized for scale.\n",
    "For larger datasets, more compute-intensive LFs, or larger LF sets, you may decide to use one of the other data formats\n",
    "that Snorkel supports natively, such as Dask DataFrames or PySpark DataFrames, and their corresponding applier objects.\n",
    "For more info, check out the [Snorkel API documentation](https://snorkel.readthedocs.io/en/master/packages/labeling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:02<00:00, 970.44it/s] \n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1226.53it/s]\n"
     ]
    }
   ],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)\n",
    "L_test = applier.apply(df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>postive</th>\n",
       "      <td>0</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.4695</td>\n",
       "      <td>0.4415</td>\n",
       "      <td>0.1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.1610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaderlex</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.6320</td>\n",
       "      <td>0.5965</td>\n",
       "      <td>0.1635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaderlex2</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>0.4715</td>\n",
       "      <td>0.4715</td>\n",
       "      <td>0.1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>posemo</th>\n",
       "      <td>4</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.1985</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>0.0490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negemo</th>\n",
       "      <td>5</td>\n",
       "      <td>[1]</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.0845</td>\n",
       "      <td>0.0775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           j Polarity  Coverage  Overlaps  Conflicts\n",
       "postive    0      [1]    0.4695    0.4415     0.1055\n",
       "negative   1      [0]    0.2725    0.2370     0.1610\n",
       "vaderlex   2   [0, 1]    0.6320    0.5965     0.1635\n",
       "vaderlex2  3   [0, 1]    0.4715    0.4715     0.1095\n",
       "posemo     4      [1]    0.1985    0.1950     0.0490\n",
       "negemo     5      [1]    0.0865    0.0845     0.0775"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "md-exclude"
    ]
   },
   "source": [
    "We see that our labeling functions vary in coverage, how much they overlap/conflict with one another, and almost certainly their accuracies as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is now to convert the labels from our LFs into a single _noise-aware_ probabilistic (or confidence-weighted) label per data point.\n",
    "A simple baseline for doing this is to take the majority vote on a per-data point basis: if more LFs voted SPAM than HAM, label it SPAM (and vice versa).\n",
    "We can test this with the\n",
    "[`MajorityLabelVoter` baseline model](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.baselines.MajorityLabelVoter.html#snorkel.labeling.model.baselines.MajorityLabelVoter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we can see from the summary statistics of our LFs in the previous section, they have varying properties and should not be treated identically. In addition to having varied accuracies and coverages, LFs may be correlated, resulting in certain signals being overrepresented in a majority-vote-based model. To handle these issues appropriately, we will instead use a more sophisticated Snorkel `LabelModel` to combine the outputs of the LFs.\n",
    "\n",
    "This model will ultimately produce a single set of noise-aware training labels, which are probabilistic or confidence-weighted labels. We will then use these labels to train a classifier for our task. For more technical details of this overall approach, see our [NeurIPS 2016](https://arxiv.org/abs/1605.07723) and [AAAI 2019](https://arxiv.org/abs/1810.02840) papers. For more info on the API, see the [`LabelModel` documentation](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.label_model.LabelModel.html#snorkel.labeling.model.label_model.LabelModel).\n",
    "\n",
    "Note that no gold labels are used during the training process.\n",
    "The only information we need is the label matrix, which contains the output of the LFs on our training set.\n",
    "The `LabelModel` is able to learn weights for the labeling functions using only the label matrix as input.\n",
    "We also specify the `cardinality`, or number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy:   71.7%\n",
      "Label Model Accuracy:     71.6%\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "from snorkel.labeling.model import LabelModel\n",
    "\n",
    "majority_model = MajorityLabelVoter()\n",
    "preds_train = majority_model.predict(L=L_train)\n",
    "\n",
    "label_model = LabelModel(cardinality=2, verbose=True)\n",
    "label_model.fit(L_train=L_train, n_epochs=500, log_freq=100, seed=123)\n",
    "\n",
    "\n",
    "Y_test = df_test.label.values\n",
    "\n",
    "majority_acc = majority_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=Y_test, tie_break_policy=\"random\")[\"accuracy\"]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority vote model or more sophisticated `LabelModel` could in principle be used directly as a classifier if the outputs of our labeling functions were made available at test time. (In this case, because of my poor LFs, LabelModel does poorly compared to MajorityLabel model, but usually, that is not the case).\n",
    "\n",
    "Anyway, these models (i.e. these re-weighted combinations of our labeling function's votes) will abstain on the data points that our labeling functions don't cover (and additionally, may require slow or unavailable features to execute at test time).\n",
    "In the next section, we will instead use the outputs of the `LabelModel` as training labels to train a discriminative classifier **which can generalize beyond the labeling function outputs** to see if we can improve performance further.\n",
    "This classifier will also only need the text of the comment to make predictions, making it much more suitable for inference over unseen comments.\n",
    "For more information on the properties of the label model, see the [Snorkel documentation](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.model.label_model.LabelModel.html#snorkel.labeling.model.label_model.LabelModel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out unlabeled data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw earlier, some of the data points in our `train` set received no labels from any of our LFs.\n",
    "These data points convey no supervision signal and tend to hurt performance, so we filter them out before training using a\n",
    "[built-in utility](https://snorkel.readthedocs.io/en/master/packages/_autosummary/labeling/snorkel.labeling.filter_unlabeled_dataframe.html#snorkel.labeling.filter_unlabeled_dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1)\n",
      "(1419, 1)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "df_train_filtered, probs_train_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_train, y=probs_train, L=L_train\n",
    ")\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section of the tutorial, we'll use the probabilistic training labels we generated in the last section to train a classifier for our task.\n",
    "**The output of the Snorkel `LabelModel` is just a set of labels which can be used with most popular libraries for performing supervised learning, such as TensorFlow, Keras, PyTorch, Scikit-Learn, Ludwig, and XGBoost.**\n",
    "In this tutorial, we use the well-known library [Scikit-Learn](https://scikit-learn.org).\n",
    "**Note that typically, Snorkel is used (and really shines!) with much more complex, training data-hungry models, but we will use Logistic Regression here for simplicity of exposition.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and speed, we use a simple \"bag of n-grams\" feature representation: each data point is represented by a one-hot vector marking which words or 2-word combinations are present in the comment text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1419, 61454)\n",
      "(1000, 61454)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 5))\n",
    "X_train = vectorizer.fit_transform(df_train_filtered.text.tolist())\n",
    "X_test = vectorizer.transform(df_test.text.tolist())\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in Section 4, the `LabelModel` outputs probabilistic (float) labels.\n",
    "If the classifier we are training accepts target labels as floats, we can train on these labels directly (see describe the properties of this type of \"noise-aware\" loss in our [NeurIPS 2016 paper](https://arxiv.org/abs/1605.07723)).\n",
    "\n",
    "If we want to use a library or model that doesn't accept probabilistic labels (such as Scikit-Learn), we can instead replace each label distribution with the label of the class that has the maximum probability.\n",
    "This can easily be done using the\n",
    "[`probs_to_preds` helper method](https://snorkel.readthedocs.io/en/master/packages/_autosummary/utils/snorkel.utils.probs_to_preds.html#snorkel.utils.probs_to_preds).\n",
    "We do note, however, that this transformation is lossy, as we no longer have values for our confidence in each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "preds_train_filtered = probs_to_preds(probs=probs_train_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use these labels to train a classifier as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": [
     "md-exclude-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance for  LogisticRegression\n",
      "Test Accuracy: 61.6%\n",
      "Performance for  LinearSVC\n",
      "Test Accuracy: 61.8%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "Y_test = df_test.label.values\n",
    "\n",
    "for classifier in [LogisticRegression(C=1e3, solver=\"liblinear\"), LinearSVC()]:\n",
    "    classifier.fit(X=X_train, y=preds_train_filtered)\n",
    "    print(\"Performance for \", type(classifier).__name__), \n",
    "    print(f\"Test Accuracy: {classifier.score(X=X_test, y=test_labels) * 100:.1f}%\")\n",
    "    #print(preds_train_filtered)\n",
    "    #print(test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This obviously looks worser than the majority labeler, but as you will see in the withLabeledTrainingData notebook, the performance is much better if we use sbert features instead of Bag of Ngrams. Let us save this new labeled dataset created by snorkel, and use it later to compare with other approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect the new dataset and save it. \n",
    "auto_labeled_data = DataFrame(\n",
    "    {'text': df_train_filtered.text.tolist(),\n",
    "     'sentiment': preds_train_filtered,\n",
    "    })\n",
    "auto_labeled_data.to_csv(\"../files/snorkellabeled_train.csv\", sep=\"\\t\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all",
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
