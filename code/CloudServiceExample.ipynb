{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates one way to address the \"no training data\" scenario in NLP. If there is a cloud based service provider offering the kind of NLP system you are looking for, that can be a good starting point in such cases.  \n",
    "\n",
    "As discussed in the talk, we still need some sort of an evaluation dataset. I am using the \"Sentiment Labelled Sentences](http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences) dataset from UCI repository. This dataset consists of 500 positive and 500 negative sentiment sentences for each of the three sources: amazon.com, yelp.com and imdb.com. I split them into two groups: train (imdb+yelp) and test (amazon). Since this is a tutorial, and not a real world scenario, I am using the **test** dataset to evaluate Azure's sentiment analysis API. I won't use the training data in this example, as our goal is to use the off the shelf services and check their efficiency. \n",
    "\n",
    "\n",
    "To setup Azure for using this service, [visit this page and follow instructions](https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/quickstarts/client-libraries-rest-api?tabs=version-3-1&pivots=programming-language-python). I am using their free tier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"XXXXXXXXXX\"\n",
    "endpoint = \"XXXXXXX\"\n",
    "#enter your own key/endpoint here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_client():\n",
    "    ta_credential = AzureKeyCredential(key)\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "            endpoint=endpoint, credential=ta_credential)\n",
    "    return text_analytics_client\n",
    "\n",
    "client = authenticate_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing  50  sentences\n",
      "Completed processing  100  sentences\n",
      "Completed processing  150  sentences\n",
      "Completed processing  200  sentences\n",
      "Completed processing  250  sentences\n",
      "Completed processing  300  sentences\n",
      "Completed processing  350  sentences\n",
      "Completed processing  400  sentences\n",
      "Completed processing  450  sentences\n",
      "Completed processing  500  sentences\n",
      "Completed processing  550  sentences\n",
      "Completed processing  600  sentences\n",
      "Completed processing  650  sentences\n",
      "Completed processing  700  sentences\n",
      "Completed processing  750  sentences\n",
      "Completed processing  800  sentences\n",
      "Completed processing  850  sentences\n",
      "Completed processing  900  sentences\n",
      "Completed processing  950  sentences\n",
      "Completed processing  1000  sentences\n"
     ]
    }
   ],
   "source": [
    "#read in the test data, and send it to azure analytics API for sentiment analysis.\n",
    "\n",
    "testfilepath = \"../files/test_labelled.txt\" #tab seperated file.\n",
    "sentences = []\n",
    "sentiments = [] #0 is negative, 1 is positive\n",
    "preds = [] #0 neg, 1 positive, 2 neutral or mixed\n",
    "preds_dict = {'positive':1, 'negative':0, 'neutral':2, 'mixed':2}\n",
    "count = 0\n",
    "for line in open(testfilepath):\n",
    "    sentence, sentiment = line.strip().split(\"\\t\")\n",
    "    pred = preds_dict[client.analyze_sentiment(documents = [sentence])[0].sentiment]\n",
    "    preds.append(pred)\n",
    "    sentences.append(sentence)\n",
    "    sentiments.append(int(sentiment))\n",
    "    \n",
    "    #adding this part to check how far did it go.\n",
    "    count +=1\n",
    "    if count%50 ==0:\n",
    "        print(\"Completed processing \", count, \" sentences\")\n",
    "        \n",
    "#Note: You can send batch requests to Azure, instead of sending one after another\n",
    "#like I am doing, although there is a limit on the number of items per batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "#rite all predictions to a file as a backup\n",
    "print(len(sentences), len(sentiments), len(preds))\n",
    "\n",
    "fw=open(\"actual-preds.csv\", \"w\") \n",
    "fw.write(\"actual,predicted\"+\"\\n\")\n",
    "for i in range(0,1000):\n",
    "    fw.write(str(sentiments[i])+\",\"+str(preds[i]))\n",
    "    fw.write(\"\\n\")\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.81      0.88       500\n",
      "           1       0.93      0.88      0.90       500\n",
      "           2       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.84      1000\n",
      "   macro avg       0.63      0.56      0.59      1000\n",
      "weighted avg       0.95      0.84      0.89      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vajjalas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\vajjalas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\vajjalas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Look at how good Azure's API was, comparing its predictions with the actual sentiment categories in this dataset. \n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(sentiments, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 84% accuracy without building any sentiment classifier ourselves! That is a great starting point (however, keep in mind, we don't find such readymade solutions to our real world problems most of the time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[405  34  61]\n",
      " [ 15 439  46]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "#Let us look at the confusion matrix for Azure's predictions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(sentiments,preds,labels=[0,1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 473, 0: 420, 2: 107})\n"
     ]
    }
   ],
   "source": [
    "#Just how many of our sentences were predicted as \"neutral\" or \"mixed\"?\n",
    "import collections\n",
    "print(collections.Counter(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, over 10% of our sentences are identified as neither positive nor negative by Azure's API. If this was a case with our real world scenario, this is perhaps not a good long term option, although it is a good starting point when we don't have any data. Perhaps one can use this as a solution for this problem in a short term, and move to weak supervision/semi-supervised/active learning soon? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
